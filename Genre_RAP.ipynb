{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c664878-4aea-49c8-bdcd-0c6a10e83c42",
   "metadata": {},
   "source": [
    "<h1> GENRE: RAP </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb58b5aa-e4aa-47ea-8de6-39736199bc11",
   "metadata": {},
   "source": [
    "<h3>(1) Access the data from the dataset </h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c36f509d-632c-432c-853f-e7dfee496973",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "%pip install datasets\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"sebastiandizon/genius-song-lyrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a84645ae-05c1-47c2-af63-c43cdf167dca",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7df9ba2ebbbf46a783a5690a72fd8517",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/5134856 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70s Done\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2f533c3505e43f69cf1f487c96c5ed8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/5134856 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80s Done\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "178e8adaca724c13a4d1a81504cdf7f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/5134856 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m subdataset_80 \u001b[38;5;241m=\u001b[39m ds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;28;01mlambda\u001b[39;00m song: \u001b[38;5;241m1980\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m song[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1989\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m song[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtag\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m type_of_music)\u001b[38;5;241m.\u001b[39msort(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mviews\u001b[39m\u001b[38;5;124m'\u001b[39m, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m80s Done\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m subdataset_90 \u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msong\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1990\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msong\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1999\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msong\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtag\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtype_of_music\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msort(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mviews\u001b[39m\u001b[38;5;124m'\u001b[39m, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m90s Done\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m subdataset_00 \u001b[38;5;241m=\u001b[39m ds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;28;01mlambda\u001b[39;00m song: \u001b[38;5;241m2000\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m song[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2009\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m song[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtag\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m type_of_music)\u001b[38;5;241m.\u001b[39msort(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mviews\u001b[39m\u001b[38;5;124m'\u001b[39m, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:560\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    558\u001b[0m }\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/fingerprint.py:442\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    438\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[1;32m    440\u001b[0m \u001b[38;5;66;03m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:  \u001b[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:3688\u001b[0m, in \u001b[0;36mDataset.filter\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3685\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   3686\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m-> 3688\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mget_indices_from_mask_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3692\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3695\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3696\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3697\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3699\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3700\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mindices\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mValue\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muint64\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3701\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3702\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3703\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3710\u001b[0m \u001b[43m    \u001b[49m\u001b[43msuffix_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffix_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_fingerprint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3712\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFilter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3715\u001b[0m new_dataset \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   3716\u001b[0m new_dataset\u001b[38;5;241m.\u001b[39m_indices \u001b[38;5;241m=\u001b[39m indices\u001b[38;5;241m.\u001b[39mdata\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:560\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    558\u001b[0m }\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:3073\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3067\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3068\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3069\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3070\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3071\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3072\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3073\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3074\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3075\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:3476\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3472\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m   3473\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mslice\u001b[39m(i, i \u001b[38;5;241m+\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mindices(shard\u001b[38;5;241m.\u001b[39mnum_rows)))\n\u001b[1;32m   3474\u001b[0m )  \u001b[38;5;66;03m# Something simpler?\u001b[39;00m\n\u001b[1;32m   3475\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3476\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_same_num_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3480\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3481\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3482\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[1;32m   3483\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[1;32m   3484\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3485\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:3338\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3336\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3337\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3338\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3340\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3341\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3342\u001b[0m     }\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:6321\u001b[0m, in \u001b[0;36mget_indices_from_mask_function\u001b[0;34m(function, batched, with_indices, with_rank, input_columns, indices_mapping, *args, **fn_kwargs)\u001b[0m\n\u001b[1;32m   6319\u001b[0m num_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(batch\u001b[38;5;241m.\u001b[39mkeys()))])\n\u001b[1;32m   6320\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_examples):\n\u001b[0;32m-> 6321\u001b[0m     example \u001b[38;5;241m=\u001b[39m {key: batch[key][i] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m batch}\n\u001b[1;32m   6322\u001b[0m     additional_args \u001b[38;5;241m=\u001b[39m ()\n\u001b[1;32m   6323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m with_indices:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:6321\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   6319\u001b[0m num_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(batch\u001b[38;5;241m.\u001b[39mkeys()))])\n\u001b[1;32m   6320\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_examples):\n\u001b[0;32m-> 6321\u001b[0m     example \u001b[38;5;241m=\u001b[39m {key: \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m[i] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m batch}\n\u001b[1;32m   6322\u001b[0m     additional_args \u001b[38;5;241m=\u001b[39m ()\n\u001b[1;32m   6323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m with_indices:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/formatting/formatting.py:279\u001b[0m, in \u001b[0;36mLazyDict.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    277\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[key]\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys_to_format:\n\u001b[0;32m--> 279\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys_to_format\u001b[38;5;241m.\u001b[39mremove(key)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/formatting/formatting.py:382\u001b[0m, in \u001b[0;36mLazyBatch.format\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m--> 382\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_column\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpa_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/formatting/formatting.py:448\u001b[0m, in \u001b[0;36mPythonFormatter.format_column\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_column\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m--> 448\u001b[0m     column \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython_arrow_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    449\u001b[0m     column \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_features_decoder\u001b[38;5;241m.\u001b[39mdecode_column(column, pa_table\u001b[38;5;241m.\u001b[39mcolumn_names[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m column\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/formatting/formatting.py:148\u001b[0m, in \u001b[0;36mPythonArrowExtractor.extract_column\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_column\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m--> 148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpa_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pylist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from heapq import nlargest\n",
    "from datasets import Dataset\n",
    "\n",
    "# Assuming the dataset has a 'category' column\n",
    "unique_categories = ds['train'].unique('tag')  # Get all unique categories\n",
    "\n",
    "# Group into subdatasets\n",
    "subdatasets = {category: ds['train'].filter(lambda x: x['tag'] == category and x['language'] == 'en') for category in unique_categories}\n",
    "\n",
    "type_of_music = 'rap'\n",
    "# Group into music decades\n",
    "#subdataset_70_without_sort = ds['train'].filter(lambda song: 1960 <= song[\"year\"] <= 1970 and song['tag'] == type_of_music)\n",
    "\n",
    "subdataset_70 = ds['train'].filter(lambda song: 1970 <= song[\"year\"] <= 1979 and song['tag'] == type_of_music).sort('views', reverse=True)\n",
    "print(\"70s Done\")\n",
    "subdataset_80 = ds['train'].filter(lambda song: 1980 <= song[\"year\"] <= 1989 and song['tag'] == type_of_music).sort('views', reverse=True).select(range(1000))\n",
    "print(\"80s Done\")\n",
    "subdataset_90 = ds['train'].filter(lambda song: 1990 <= song[\"year\"] <= 1999 and song['tag'] == type_of_music).sort('views', reverse=True).select(range(1000))\n",
    "print(\"90s Done\")\n",
    "subdataset_00 = ds['train'].filter(lambda song: 2000 <= song[\"year\"] <= 2009 and song['tag'] == type_of_music).sort('views', reverse=True).select(range(1000))\n",
    "print(\"00s Done\")\n",
    "#subdataset_10 = ds['train'].filter(lambda song: 2010 <= song[\"year\"] <= 2020 and song['tag'] == type_of_music).sort('views', reverse=True).select(range(1000))\n",
    "filtered_ds = ds['train'].filter(\n",
    "    lambda song: 2010 <= song[\"year\"] <= 2019 and song['tag'] == type_of_music,\n",
    "    num_proc=4\n",
    ")\n",
    "\n",
    "# 2) Umwandeln in eine Python-Liste\n",
    "all_filtered = list(filtered_ds)\n",
    "\n",
    "# 3) Nur die Top-1000 nach 'views' herausholen\n",
    "top_1000 = nlargest(1000, all_filtered, key=lambda x: x[\"views\"])\n",
    "\n",
    "# 4) (Optional) Zurück in ein Dataset\n",
    "subdataset_10 = Dataset.from_list(top_1000)\n",
    "print(\"10s Done\")\n",
    "#subdataset_30 = ds['train'].filter(lambda song: 2020 <= song[\"year\"] <= 2030 and song['tag'] == type_of_music).sort('views', reverse=True).select(range(1000))\n",
    "filtered_ds = ds['train'].filter(\n",
    "    lambda song: 2020 <= song[\"year\"] <= 2029 and song['tag'] == type_of_music,\n",
    "    num_proc=4\n",
    ")\n",
    "\n",
    "# 2) Umwandeln in eine Python-Liste\n",
    "all_filtered = list(filtered_ds)\n",
    "\n",
    "# 3) Nur die Top-1000 nach 'views' herausholen\n",
    "top_1000 = nlargest(1000, all_filtered, key=lambda x: x[\"views\"])\n",
    "\n",
    "# 4) (Optional) Zurück in ein Dataset\n",
    "subdataset_20 = Dataset.from_list(top_1000)\n",
    "print(\"20s Done\")\n",
    "\n",
    "# Create subdataset\n",
    "#eng_rap = subdatasets['rap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "19708af4",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "757e9ec0ac8a4cd2945025091e72c148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/217 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e155511e04894234b75ed95ef9cf7eaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f47c9d39dd704234817488bcedb5ef32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91b9e84bb7054f44993d2c9e69d3dcfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9a9325c32c9477fb95a0c2b2522a43d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3c81240e130403a958d17332b392757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import heapq\n",
    "from datasets import Dataset\n",
    "type_of_music= 'rap'\n",
    "TOP_K = 1000\n",
    "decade_heaps = {70: [], 80: [], 90: [], 00: [], 10:[], 20:[]}  # z.B. 70=70er, 0=00er etc.\n",
    "\n",
    "def get_decade(year):\n",
    "    if 1970 <= year <= 1979:\n",
    "        return 70\n",
    "    elif 1980 <= year <= 1989:\n",
    "        return 80\n",
    "    elif 1990 <= year <= 1999:\n",
    "        return 90\n",
    "    elif 2000 <= year <= 2009:\n",
    "        return 00\n",
    "    elif 2010 <= year <= 2019:\n",
    "        return 10\n",
    "    elif 2020 <= year <= 2029:\n",
    "        return 20\n",
    "    return None\n",
    "\n",
    "i = 0  # Zähler für tie-break\n",
    "for song in ds[\"train\"]:\n",
    "    if song[\"tag\"] != type_of_music:\n",
    "        continue\n",
    "    elif song[\"language\"] != \"en\":\n",
    "        continue\n",
    "    dec = get_decade(song[\"year\"])\n",
    "    if dec is None:\n",
    "        continue\n",
    "\n",
    "    views = song[\"views\"]\n",
    "    heap = decade_heaps[dec]\n",
    "\n",
    "    # Das Tupel enthält jetzt (views, i, song).\n",
    "    # Der zweite Wert i ist rein für Tie-Break (und garantiert sortierbar).\n",
    "    if len(heap) < TOP_K:\n",
    "        heapq.heappush(heap, (views, i, song))\n",
    "    else:\n",
    "        # Vergleiche nur, ob aktuelle views (oder Tie-Break i) größer sind \n",
    "        # als das Minimum im Heap (heap[0]).\n",
    "        if (views, i) > (heap[0][0], heap[0][1]):\n",
    "            heapq.heapreplace(heap, (views, i, song))\n",
    "    i += 1\n",
    "\n",
    "# Am Ende hat jede Dekade max. 1000 Elemente.\n",
    "decade_toplists = {}\n",
    "for dec, heap in decade_heaps.items():\n",
    "    # Sortieren nach (views, i) absteigend.\n",
    "    # Falls dir der Index egal ist, kannst du einfach nur nach views sortieren.\n",
    "    sorted_heap = sorted(heap, key=lambda x: (x[0], x[1]), reverse=True)\n",
    "    # x[0] = views, x[1] = tie-break index, x[2] = song\n",
    "    songs_only = [item[2] for item in sorted_heap]\n",
    "    decade_toplists[dec] = songs_only\n",
    "\n",
    "# Optional: zu HF Dataset konvertieren\n",
    "subdataset_70 = Dataset.from_list(decade_toplists[70])\n",
    "subdataset_80 = Dataset.from_list(decade_toplists[80])\n",
    "subdataset_90 = Dataset.from_list(decade_toplists[90])\n",
    "subdataset_00 = Dataset.from_list(decade_toplists[00])\n",
    "subdataset_10 = Dataset.from_list(decade_toplists[10])\n",
    "subdataset_20 = Dataset.from_list(decade_toplists[20])\n",
    "\n",
    "# Save to hard drive\n",
    "subdataset_70.save_to_disk(\"subdataset_70_rap\")\n",
    "subdataset_80.save_to_disk(\"subdataset_80_rap\")\n",
    "subdataset_90.save_to_disk(\"subdataset_90_rap\")\n",
    "subdataset_00.save_to_disk(\"subdataset_00_rap\")\n",
    "subdataset_10.save_to_disk(\"subdataset_10_rap\")\n",
    "subdataset_20.save_to_disk(\"subdataset_20_rap\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eca33b8-ff6c-4ab3-aae9-8de32db0f00e",
   "metadata": {},
   "source": [
    "<h2> LDA </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6c1930-379d-4c13-8378-0c7e31382765",
   "metadata": {},
   "source": [
    "<h2> Remove the bad words</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0155b7f",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Stan',\n",
       " 'tag': 'rap',\n",
       " 'artist': 'Eminem',\n",
       " 'year': 2000,\n",
       " 'views': 3298983,\n",
       " 'features': '{Dido}',\n",
       " 'lyrics': '[Produced by DJ Mark the 45 King]\\n\\n[Chorus: Dido]\\nMy tea\\'s gone cold, I\\'m wondering why I\\nGot out of bed at all\\nThe morning rain clouds up my window\\nAnd I can\\'t see at all\\nAnd even if I could it\\'d all be gray\\nBut your picture on my wall\\nIt reminds me that it\\'s not so bad, it\\'s not so bad\\nMy tea\\'s gone cold, I\\'m wondering why I\\nGot out of bed at all\\nThe morning rain clouds up my window\\nAnd I can\\'t see at all\\nAnd even if I could it\\'d all be gray\\nBut your picture on my wall\\nIt reminds me that it\\'s not so bad, it\\'s not so bad\\n\\n[Verse 1: Eminem]\\nDear Slim, I wrote you, but you still ain\\'t callin\\'\\nI left my cell, my pager and my home phone at the bottom\\nI sent two letters back in autumn, you must not\\'ve got \\'em\\nThere probably was a problem at the post office or somethin\\'\\nSometimes I scribble addresses too sloppy when I jot \\'em\\nBut anyways, fuck it, what\\'s been up, man? How\\'s your daughter?\\nMy girlfriend\\'s pregnant too, I\\'m \\'bout to be a father\\nIf I have a daughter, guess what I\\'ma call her?\\nI\\'ma name her Bonnie\\nI read about your Uncle Ronnie too, I\\'m sorry\\nI had a friend kill himself over some bitch who didn\\'t want him\\nI know you probably hear this every day, but I\\'m your biggest fan\\nI even got the underground shit that you did with Skam\\nI got a room full of your posters and your pictures, man\\nI like the shit you did with Rawkus too, that shit was phat\\nAnyways, I hope you get this, man, hit me back\\nJust to chat, truly yours, your biggest fan, this is Stan\\n[Chorus: Dido]\\nMy tea\\'s gone cold, I\\'m wondering why I\\nGot out of bed at all\\nThe morning rain clouds up my window\\nAnd I can\\'t see at all\\nAnd even if I could it\\'d all be gray\\nBut your picture on my wall\\nIt reminds me that it\\'s not so bad, it\\'s not so bad\\n\\n[Verse 2: Eminem]\\nDear Slim, you still ain\\'t called or wrote, I hope you have a chance\\nI ain\\'t mad, I just think it\\'s fucked up you don\\'t answer fans\\nIf you didn\\'t want to talk to me outside your concert, you didn\\'t have to\\nBut you coulda signed an autograph for Matthew\\nThat\\'s my little brother, man, he\\'s only six years old\\nWe waited in the blisterin\\' cold\\nFor you, for four hours, and you just said no\\nThat\\'s pretty shitty, man, you\\'re like his fuckin\\' idol\\nHe wants to be just like you, man, he likes you more than I do\\nI ain\\'t that mad, though I just don\\'t like bein\\' lied to\\nRemember when we met in Denver?\\nYou said if I\\'d write you, you would write back\\nSee, I\\'m just like you in a way: I never knew my father neither\\nHe used to always cheat on my mom and beat her\\nI can relate to what you\\'re sayin\\' in your songs\\nSo when I have a shitty day, I drift away and put \\'em on\\n‘Cause I don\\'t really got shit else\\nSo that shit helps when I\\'m depressed\\nI even got a tattoo with your name across the chest\\nSometimes I even cut myself to see how much it bleeds\\nIt\\'s like adrenaline, the pain is such a sudden rush for me\\nSee, everything you say is real, and I respect you ‘cause you tell it\\nMy girlfriend\\'s jealous \\'cause I talk about you 24/7\\nBut she don\\'t know you like I know you, Slim, no one does\\nShe don\\'t know what it was like for people like us growin\\' up\\nYou gotta call me, man, I\\'ll be the biggest fan you\\'ll ever lose\\nSincerely yours, Stan—P.S. We should be together too\\n[Chorus: Dido]\\nMy tea\\'s gone cold, I\\'m wondering why I\\nGot out of bed at all\\nThe morning rain clouds up my window\\nAnd I can\\'t see at all\\nAnd even if I could it\\'d all be gray\\nBut your picture on my wall\\nIt reminds me that it\\'s not so bad, it\\'s not so bad\\n\\n[Verse 3: Eminem]\\nDear Mr. I\\'m-Too-Good-to-Call-or-Write-My-Fans\\nThis\\'ll be the last package I ever send your ass\\nIt\\'s been six months, and still no word—I don\\'t deserve it?\\nI know you got my last two letters, I wrote the addresses on \\'em perfect\\nSo this is my cassette I\\'m sendin\\' you, I hope you hear it\\nI\\'m in the car right now, I\\'m doin\\' 90 on the freeway\\nHey, Slim, I drank a fifth of vodka, you dare me to drive?\\nYou know the song by Phil Collins, \"In the Air of the Night\"\\nAbout that guy who coulda saved that other guy from drownin\\'\\nBut didn\\'t, then Phil saw it all, then at a show he found him?\\nThat\\'s kinda how this is: you coulda rescued me from drownin\\'\\nNow it\\'s too late, I\\'m on a thousand downers now—I\\'m drowsy\\nAnd all I wanted was a lousy letter or a call\\nI hope you know I ripped all of your pictures off the wall\\nI loved you, Slim, we coulda been together—think about it!\\nYou ruined it now, I hope you can\\'t sleep and you dream about it\\nAnd when you dream I hope you can\\'t sleep and you scream about it\\nI hope your conscience eats at you and you can\\'t breathe without me\\nSee, Slim—shut up, bitch! I\\'m tryin\\' to talk\\nHey, Slim, that\\'s my girlfriend screamin\\' in the trunk\\nBut I didn\\'t slit her throat, I just tied her up—see? I ain\\'t like you\\n‘Cause if she suffocates she\\'ll suffer more and then she\\'ll die too\\nWell, gotta go, I\\'m almost at the bridge now\\nOh, shit, I forgot—how am I supposed to send this shit out?!\\n[Chorus: Dido]\\nMy tea\\'s gone cold, I\\'m wondering why I\\nGot out of bed at all\\nThe morning rain clouds up my window\\nAnd I can\\'t see at all\\nAnd even if I could it\\'d all be gray\\nBut your picture on my wall\\nIt reminds me that it\\'s not so bad, it\\'s not so bad\\n\\n[Verse 4: Eminem]\\nDear Stan, I meant to write you sooner, but I just been busy\\nYou said your girlfriend\\'s pregnant now, how far along is she?\\nLook, I\\'m really flattered you would call your daughter that\\nAnd here\\'s an autograph for your brother; I wrote it on a Starter cap\\nI\\'m sorry I didn\\'t see you at the show, I must\\'ve missed you\\nDon\\'t think I did that shit intentionally just to diss you\\nBut what\\'s this shit you said about you like to cut your wrists too?\\nI say that shit just clownin\\', dawg, come on, how fucked up is you?\\nYou got some issues, Stan, I think you need some counselin\\'\\nTo help your ass from bouncin\\' off the walls when you get down some\\nAnd what\\'s this shit about us meant to be together?\\nThat type of shit\\'ll make me not want us to meet each other\\nI really think you and your girlfriend need each other\\nOr maybe you just need to treat her better\\nI hope you get to read this letter, I just hope it reaches you in time\\nBefore you hurt yourself, I think that you\\'ll be doin\\' just fine\\nIf you relax a little, I\\'m glad I inspire you, but Stan\\nWhy are you so mad? Try to understand that I do want you as a fan\\nI just don\\'t want you to do some crazy shit\\nI seen this one shit on the news a couple weeks ago that made me sick\\nSome dude was drunk and drove his car over a bridge\\nAnd had his girlfriend in the trunk, and she was pregnant with his kid\\nAnd in the car they found a tape, but they didn\\'t say who it was to\\nCome to think about it, his name was—it was you\\n\\n[Outro]\\nDamn…',\n",
       " 'id': 772,\n",
       " 'language_cld3': 'en',\n",
       " 'language_ft': 'en',\n",
       " 'language': 'en'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subdataset_00[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc19c9ee-4499-49cb-ba77-32ff6fa3fb0c",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/maurice/.cache/kagglehub/datasets/sahib12/badwords/versions/1\n",
      "!!!!Loaded 2130 swear words.\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import os\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"tushifire/ldnoobw\")\n",
    "path = kagglehub.dataset_download(\"sahib12/badwords\")\n",
    "\n",
    "#file = kagglehub.load_dataset(\"tushifire/ldnoobw\",path)\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "swear_words_file = os.path.join(path, 'Terms-to-Block.csv')  # Replace 'en.txt' with the correct filename\n",
    "\n",
    "with open(swear_words_file, 'r') as f:\n",
    "    swear_words = set(f.read().splitlines())\n",
    "\n",
    "print(f\"!!!!Loaded {len(swear_words)} swear words.\")\n",
    "\n",
    "#print(swear_words)\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cecd6a07-2735-4e22-8936-6201e2f4454e",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords: {'will', 'as', 'those', 'wasn', 'aren', 'no', 'your', 'outro', 'couldn', \"haven't\", 'oh', 'is', 'ma', 'themselves', 'all', 'weren', \"you'd\", \"don't\", 'needn', 'if', 'yours', 'under', 'any', 'then', 'of', 'further', 'yourselves', 'and', 've', 'do', 'had', 'you', 'ain', \"it's\", 'this', 'about', 'how', \"mustn't\", 'won', 'doesn', \"wasn't\", 'her', 'not', \"aren't\", 'bridge', 'have', 'so', 'before', 'am', 'ourselves', 'more', 't', 'shouldn', 'an', 'd', 'than', 'she', 'just', 'there', \"weren't\", 'on', 'll', 'why', 'wouldn', 'by', 'whom', 'i', \"shan't\", 'that', \"hasn't\", 'too', 'myself', 'were', 'once', \"shouldn't\", 'but', 'the', \"isn't\", \"didn't\", \"needn't\", 'o', 'now', 'has', 'very', 'they', 'having', 'was', 'me', 'itself', 'hasn', 'for', 'below', 'chorus', 'above', 'himself', 'didn', \"she's\", 'be', 'into', \"should've\", 'theirs', 'from', 's', 'both', 'mustn', 'a', 'my', 'yourself', 'between', 'to', 'some', 'who', 'y', 'own', 'because', 'same', 'doing', \"won't\", 'get', 'it', 'our', 'got', 'while', 'don', 'hers', 'them', 'again', 'off', 'or', 'through', 're', 'out', 'nor', 'are', 'herself', 'does', \"couldn't\", 'yeah', 'up', 'until', \"that'll\", \"you're\", 'when', 'should', 'after', 'shan', 'did', 'm', \"doesn't\", 'mightn', 'these', 'most', 'hadn', \"mightn't\", 'in', 'their', 'his', 'during', 'other', 'few', 'what', \"hadn't\", 'like', 'haven', 'ours', \"you'll\", 'over', 'such', 'being', 'at', \"you've\", 'which', 'here', 'isn', 'yo', 'where', 'we', 'against', 'each', 'him', 'na', 'down', \"wouldn't\", 'hook', 'only', 'verse', 'its', 'la', 'can', 'intro', 'been', 'with', 'he'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/maurice/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/maurice/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import re\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "# Sicherstellen, dass NLTK-Daten heruntergeladen sind\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "used_swear_words = []\n",
    "# clean the text\n",
    "import re\n",
    "\n",
    "\n",
    "# Stopwords definieren\n",
    "stop_words = set(stopwords.words('english')+[\"like\", \"oh\", \"na\", \"la\", \"yo\", \"get\", \"yeah\", \"got\",\"verse\", \"chorus\", \"hook\", \"bridge\", \"outro\", \"intro\"])\n",
    "print(\"Stopwords:\", stop_words)\n",
    "\n",
    "def clean_text(document_list):\n",
    "    \n",
    "    cleaned_document_list = []\n",
    "    \n",
    "    for single_doc in document_list:\n",
    "        # Delete HTML-Tags\n",
    "        text = re.sub(r\"<.*?>\", \"\", single_doc)\n",
    "        # Delete Sonderzeichen und Zahlen\n",
    "        text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "        # Delete uncessary whitespaces\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "        \n",
    "        # lower case the text\n",
    "        text = text.lower()\n",
    "        \n",
    "        cleaned_document_list.append(text)\n",
    "    \n",
    "    return cleaned_document_list\n",
    "\n",
    "\n",
    "def lemmatize_docs(subdataset):\n",
    "    import spacy\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "    \n",
    "    lemmatized_document_list = []\n",
    "    lemmatized = []\n",
    "    \n",
    "    for single_doc in subdataset:\n",
    "        nlp_doc = nlp(single_doc)\n",
    "\n",
    "        for token in nlp_doc:\n",
    "        # Nur Tokens behalten, die:\n",
    "        # - kein Stopwort sind\n",
    "        # - kein Satzzeichen sind\n",
    "        # - eine gewisse Länge haben (z. B. > 1 Zeichen)\n",
    "            if not token.is_stop and not token.is_punct and len(token.text.strip()) > 1:\n",
    "                lemmatized.append(token.lemma_.lower())\n",
    "        \n",
    "        \n",
    "        lemmatized_document_list.append(lemmatized)\n",
    "    \n",
    "    return lemmatized_document_list\n",
    "    \n",
    "\n",
    "def get_swear_words():\n",
    "\n",
    "    path = kagglehub.dataset_download(\"tushifire/ldnoobw\")\n",
    "    path = kagglehub.dataset_download(\"sahib12/badwords\")\n",
    "\n",
    "    #file = kagglehub.load_dataset(\"tushifire/ldnoobw\",path)\n",
    "\n",
    "    print(\"Path to dataset files:\", path)\n",
    "\n",
    "    swear_words_file = os.path.join(path, 'Terms-to-Block.csv')  # Replace 'en.txt' with the correct filename\n",
    "\n",
    "    with open(swear_words_file, 'r') as f:\n",
    "        swear_words = set(f.read().splitlines())\n",
    "\n",
    "    print(f\"!!!!Loaded {len(swear_words)} swear words.\")\n",
    "\n",
    "    return swear_words\n",
    "\n",
    "def get_manual_stop_words():\n",
    "    \n",
    "    path = '/home/maurice/Dokumente/3rd_Semester_LiU/TDDE16_Projekt/TDDE16_Song_Lyrics_Analysis/Manually_Banned_Words.txt'\n",
    "    path_manually_stop_words = '/home/maurice/Dokumente/3rd_Semester_LiU/TDDE16_Projekt/TDDE16_Song_Lyrics_Analysis/stopwords_en.txt'\n",
    "\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        text_banned = f.read()\n",
    "    \n",
    "    with open(path_manually_stop_words, 'r', encoding='utf-8') as f:\n",
    "        text_stopwords = f.read()\n",
    "        \n",
    "    banned_words = set(text_banned.split())\n",
    "    stop_words = set(text_stopwords.split())\n",
    "    \n",
    "    return banned_words.union(stop_words)\n",
    "\n",
    "# Preprocessing: Tokenisieren, Stopwords entfernen, Kleinbuchstaben\n",
    "def preprocess_stop_words(document_list):\n",
    "    # Stopwords definieren\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))  # Standard-Stopwords\n",
    "    #additional_words = {\"like\", \"oh\", \"na\", \"la\", \"yo\", \"you\", \"get\", \"yeah\" \n",
    "    #                       \"the\", \"and\", \"of\", \"on\", \"[Intro]\", \"[Storyteller]\"}\n",
    "    \n",
    "    # import the manual files of stop words\n",
    "    additional_words = get_manual_stop_words()\n",
    "    swear_words = get_swear_words()\n",
    "      \n",
    "    word_set = stop_words.union(additional_words)\n",
    "    word_set = word_set.union(swear_words)\n",
    "        \n",
    "    replacement = \"\"\n",
    "    list_of_tokens = []\n",
    "    for single_doc in document_list:\n",
    "\n",
    "        words = single_doc.split()\n",
    "        \n",
    "        # replace word if not in the list\n",
    "        replaced_words = []\n",
    "        for word in words:\n",
    "            if word.lower() in word_set:\n",
    "                replaced_words.append(replacement)  \n",
    "                \n",
    "                if word.lower() in swear_words:\n",
    "                    used_swear_words.append(word) # save word to the used swear words list\n",
    "            else:\n",
    "                replaced_words.append(word)  \n",
    "                \n",
    "        list_of_tokens.append(str(\" \".join(replaced_words)))\n",
    "              \n",
    "    \n",
    "    return list_of_tokens\n",
    "\n",
    "def remove_repeated_lines(doc):\n",
    "\n",
    "    unique_lines = []\n",
    "    seen = set()\n",
    "    for line in doc:\n",
    "        if line not in seen:\n",
    "            unique_lines.append(line)\n",
    "            seen.add(line)\n",
    "    # Am Ende kannst du wieder eine Liste zurückgeben:\n",
    "    return unique_lines\n",
    "\n",
    "def tokenize_all_docs(document_list):\n",
    "    list_of_tokens = []\n",
    "    \n",
    "    for single_doc in document_list:\n",
    "        \n",
    "        if type(single_doc) == str:\n",
    "            tokens = word_tokenize(single_doc)\n",
    "            list_of_tokens.append(tokens)\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    return list_of_tokens\n",
    "\n",
    "def get_used_swear_words():\n",
    "    return used_swear_words\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(doc):\n",
    "    doc = re.sub(r\"[^a-zA-Z0-9\\s]\", \" \", doc) # delete unncessary things\n",
    "    #doc = remove_stopwords(doc.lower())\n",
    "    doc = remove_repeated_lines(doc)\n",
    "    \n",
    "    tokens = word_tokenize(doc.lower())  # Kleinbuchstaben und Tokenisierung\n",
    "    \n",
    "    tokens = [word for word in tokens if word not in stop_words]  # Stopwords entfernen\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def remove_repeated_lines(doc):\n",
    "    lines = doc.split(\"\\n\")\n",
    "    return \"\\n\".join(sorted(set(lines), key=lines.index))\n",
    "\n",
    "def sort_out_bad_words(doc,word_list, replacement=\" \"):\n",
    "\n",
    "    # Erstelle ein Set für einen schnelleren Lookup\n",
    "    word_set = set(word_list)\n",
    "    \n",
    "    # Splitte den Text in Wörter\n",
    "    words = doc.split()\n",
    "    \n",
    "    # Ersetze jedes Wort, falls es in der Liste vorkommt\n",
    "    replaced_words = [replacement if word.lower() in word_set else word for word in words]\n",
    "    \n",
    "    # Füge den Text wieder zusammen\n",
    "    return \" \".join(replaced_words)\n",
    " \n",
    "\n",
    "def process_subdatasets(subdataset):\n",
    "    # Get rid of the bad words and replace it with \"swear word\"\n",
    "    #without_swear = [sort_out_bad_words(doc, swear_words ) for doc in subdataset]\n",
    "\n",
    "    # Preprocessing für alle Dokumente anwenden\n",
    "    #processed_docs = [preprocess(doc) for doc in without_swear]\n",
    "\n",
    "    cleaned_dataset_no_strange_characters = clean_text(subdataset)\n",
    "\n",
    "    cleaned_dataset_no_swears = preprocess_stop_words(cleaned_dataset_no_strange_characters)\n",
    "\n",
    "    lemmatized_dataset = lemmatize_docs(cleaned_dataset_no_swears)\n",
    "    \n",
    "    #tokenized_dataset = tokenize_all_docs(lemmatized_dataset)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    #if stop_words in processed_docs:\n",
    "    #    print(\"FEHLER: Stopwords nicht entfernt!\")\n",
    "        \n",
    "    # Erstellen eines Wörterbuchs\n",
    "    #dictionary = Dictionary(processed_docs)\n",
    "    dictionary = Dictionary(lemmatized_dataset)\n",
    "    \n",
    "    # Erstellen eines Bag-of-Words-Korpus\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in lemmatized_dataset]\n",
    "\n",
    "    # LDA-Modell trainieren\n",
    "    lda_model = gensim.models.LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=dictionary,\n",
    "        num_topics=5,  # Anzahl der Themen\n",
    "        random_state=42,\n",
    "        passes=40,  # Anzahl der Durchgänge\n",
    "        alpha='symmetric',\n",
    "        eta='auto'\n",
    "        #alpha='auto'  # Dirichlet-Hyperparameter Entscheidung zwischen Auto und symetric\n",
    "    )\n",
    "    \n",
    "    id2word = dictionary\n",
    "    dict_topic = {}\n",
    "    # Themen anzeigen\n",
    "    #print(\"Themen und ihre Schlüsselwörter:\")\n",
    "    for idx, topic in lda_model.print_topics(num_words=5):\n",
    "\n",
    "        dict_topic[idx] = topic \n",
    "\n",
    "    return dict_topic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750121e6-67ce-4d46-bfb2-08910881b4c9",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/maurice/.cache/kagglehub/datasets/sahib12/badwords/versions/1\n",
      "!!!!Loaded 2130 swear words.\n",
      "70s Done\n",
      "Path to dataset files: /home/maurice/.cache/kagglehub/datasets/sahib12/badwords/versions/1\n",
      "!!!!Loaded 2130 swear words.\n",
      "80s Done\n",
      "Path to dataset files: /home/maurice/.cache/kagglehub/datasets/sahib12/badwords/versions/1\n",
      "!!!!Loaded 2130 swear words.\n",
      "90s Done\n",
      "Path to dataset files: /home/maurice/.cache/kagglehub/datasets/sahib12/badwords/versions/1\n",
      "!!!!Loaded 2130 swear words.\n",
      "Path to dataset files: /home/maurice/.cache/kagglehub/datasets/sahib12/badwords/versions/1\n",
      "!!!!Loaded 2130 swear words.\n",
      "10s Done\n",
      "Path to dataset files: /home/maurice/.cache/kagglehub/datasets/sahib12/badwords/versions/1\n",
      "!!!!Loaded 2130 swear words.\n",
      "20s Done\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Analysis_Folder/topics_rap_LDA_1000_11_1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Datei schreiben\u001b[39;00m\n\u001b[1;32m     47\u001b[0m output_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnalysis_Folder/topics_rap_LDA_1000_11_1.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     49\u001b[0m     writer \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mDictWriter(file, fieldnames\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenre\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDecade\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTopic\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     50\u001b[0m     writer\u001b[38;5;241m.\u001b[39mwriteheader()  \u001b[38;5;66;03m# Schreibe die Kopfzeile\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Analysis_Folder/topics_rap_LDA_1000_11_1.csv'"
     ]
    }
   ],
   "source": [
    "amount = 1000\n",
    "from datasets import load_from_disk\n",
    "path = \"/home/maurice/Dokumente/3rd_Semester_LiU/TDDE16_Projekt/TDDE16_Song_Lyrics_Analysis/Subdatasets/rap/\"\n",
    "\n",
    "\n",
    "rap_70_lyrics = process_subdatasets(load_from_disk(path + \"subdataset_70_rap\")['lyrics'][:amount])\n",
    "print(\"70s Done\")\n",
    "rap_80_lyrics = process_subdatasets(load_from_disk(path + \"subdataset_80_rap\")['lyrics'][:amount])\n",
    "print(\"80s Done\")\n",
    "rap_90_lyrics = process_subdatasets(load_from_disk(path + \"subdataset_90_rap\")['lyrics'][:amount])\n",
    "print(\"90s Done\")\n",
    "rap_00_lyrics = process_subdatasets(load_from_disk(path + \"subdataset_00_rap\")['lyrics'][:amount])\n",
    "print(\"00s Done\")\n",
    "rap_10_lyrics = process_subdatasets(load_from_disk(path + \"subdataset_10_rap\")['lyrics'][:amount])\n",
    "print(\"10s Done\")\n",
    "rap_20_lyrics = process_subdatasets(load_from_disk(path + \"subdataset_20_rap\")['lyrics'][:amount])\n",
    "print(\"20s Done\")\n",
    "#rap_30_lyrics = process_subdatasets(subdataset_30['lyrics'][:amount])\n",
    "\n",
    "#list_of_lyrics = [rap_70_lyrics,rap_80_lyrics]\n",
    "list_of_lyrics = {\n",
    "    \"70s\": rap_70_lyrics,\n",
    "    \"80s\": rap_80_lyrics,\n",
    "    \"90s\": rap_90_lyrics,\n",
    "    \"00s\": rap_00_lyrics,\n",
    "    \"10s\": rap_10_lyrics,\n",
    "    \"20s\": rap_20_lyrics\n",
    "}\n",
    "\n",
    "import csv\n",
    "\n",
    "data_for_csv = []\n",
    "\n",
    "for decade_name, decade in list_of_lyrics.items():\n",
    "    i = 0\n",
    "    for topics in decade:\n",
    "        i = i + 1\n",
    "        data_for_csv.append({\n",
    "            \"Method\": \"LDA\",\n",
    "            \"Genre\": \"Rap\",\n",
    "            \"Decade\": decade_name,\n",
    "            \"Topic\": str(decade[topics])\n",
    "        })\n",
    "\n",
    "path = \"/home/maurice/Dokumente/3rd_Semester_LiU/TDDE16_Projekt/TDDE16_Song_Lyrics_Analysis/\"\n",
    "\n",
    "# Datei schreiben\n",
    "output_file = \"Analysis_Folder/topics_rap_LDA_1000_11_1.csv\"\n",
    "with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\"Method\", \"Genre\", \"Decade\", \"Topic\"])\n",
    "    writer.writeheader()  # Schreibe die Kopfzeile\n",
    "    writer.writerows(data_for_csv)  # Schreibe die Daten\n",
    "\n",
    "print(f\"CSV-Datei '{output_file}' wurde erfolgreich erstellt!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea2485e",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV-Datei '/home/maurice/Dokumente/3rd_Semester_LiU/TDDE16_Projekt/TDDE16_Song_Lyrics_Analysis/Analysis_Folder/topics_rap_LDA_1000_11_1.csv' wurde erfolgreich erstellt!\n"
     ]
    }
   ],
   "source": [
    "path = \"/home/maurice/Dokumente/3rd_Semester_LiU/TDDE16_Projekt/TDDE16_Song_Lyrics_Analysis/\"\n",
    "# Datei schreiben\n",
    "output_file = path + \"Analysis_Folder/topics_rap_LDA_1000_11_1.csv\"\n",
    "with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\"Method\", \"Genre\", \"Decade\", \"Topic\"])\n",
    "    writer.writeheader()  # Schreibe die Kopfzeile\n",
    "    writer.writerows(data_for_csv)  # Schreibe die Daten\n",
    "\n",
    "print(f\"CSV-Datei '{output_file}' wurde erfolgreich erstellt!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85796d1",
   "metadata": {},
   "source": [
    "BiGrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "468b6e26",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Input Token\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "\n",
    "def calculate_with_bigrams(subdataset):\n",
    "    # Phrases braucht eine Liste von tokenisierten Sätzen\n",
    "    # mindestens in 2 Dokuemnten min_count=2\n",
    "    # threshold= strikt, wie oft ein bigram vorkommen muss (höher)\n",
    "    \n",
    "    cleaned_dataset_no_strange_characters = clean_text(subdataset)\n",
    "\n",
    "    cleaned_dataset_no_swears = preprocess_stop_words(cleaned_dataset_no_strange_characters)\n",
    "\n",
    "    lemmatized_dataset = lemmatize_docs(cleaned_dataset_no_swears)\n",
    "    \n",
    "    #tokenized_dataset = tokenize_all_docs(lemmatized_dataset)\n",
    "    tokenized_dataset=lemmatized_dataset\n",
    "    \n",
    "    bigram = Phrases(tokenized_dataset, min_count=1, threshold=1)\n",
    "\n",
    "\n",
    "    bigram_phraser = Phraser(bigram)\n",
    "\n",
    "    tokenized_corpus_bigrams = [bigram_phraser[sent] for sent in tokenized_dataset]\n",
    "    #for sent in tokenized_corpus_bigrams[:5]:\n",
    "    #    print(\"Nach Bigram-Phraser:\", sent)\n",
    "    #    print(\"-----\")\n",
    "\n",
    "\n",
    "    # 1) Dictionary\n",
    "    dictionary = Dictionary(tokenized_corpus_bigrams)\n",
    "\n",
    "    # 2) Bag-of-Words\n",
    "    corpus_bow = [dictionary.doc2bow(doc) for doc in tokenized_corpus_bigrams]\n",
    "\n",
    "    # 3) LDA trainieren\n",
    "    lda_model = LdaModel(corpus=corpus_bow,\n",
    "                        id2word=dictionary,\n",
    "                        num_topics=2,  # Beispiel, wähle was passt\n",
    "                        passes=10,     # Mehrfache Durchläufe\n",
    "                        random_state=42)\n",
    "\n",
    "    # Ergebnis inspizieren\n",
    "    #for idx, topic in lda_model.show_topics(formatted=False, num_words=5):\n",
    "    #    print(f\"Topic {idx}: {[word for word, _ in topic]}\")\n",
    "        \n",
    "    dict_topic = {}\n",
    "    for idx, topic in lda_model.print_topics(num_words=10):\n",
    "\n",
    "        dict_topic[idx] = topic \n",
    "\n",
    "    return dict_topic\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fd18ed",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "amount = 1000\n",
    "from datasets import load_from_disk\n",
    "\n",
    "path = \"/home/maurice/Dokumente/3rd_Semester_LiU/TDDE16_Projekt/TDDE16_Song_Lyrics_Analysis/Subdatasets/rap/\"\n",
    "\n",
    "\n",
    "rap_70_lyrics = calculate_with_bigrams(load_from_disk(path + 'subdataset_70_rap')['lyrics'][:amount])\n",
    "print(\"Done\")\n",
    "rap_80_lyrics = calculate_with_bigrams(load_from_disk(path + \"subdataset_80_rap\")['lyrics'][:amount])\n",
    "print(\"Done\")\n",
    "rap_90_lyrics = calculate_with_bigrams(load_from_disk(path + \"subdataset_90_rap\")['lyrics'][:amount])\n",
    "print(\"Done\")\n",
    "rap_00_lyrics = calculate_with_bigrams(load_from_disk(path + \"subdataset_00_rap\")['lyrics'][:amount])\n",
    "print(\"Done\")\n",
    "rap_10_lyrics = calculate_with_bigrams(load_from_disk(path + \"subdataset_10_rap\")['lyrics'][:amount])\n",
    "print(\"Done\")\n",
    "rap_20_lyrics = calculate_with_bigrams(load_from_disk(path + \"subdataset_20_rap\")['lyrics'][:amount])\n",
    "print(\"Completly Done\")\n",
    "\n",
    "list_of_lyrics = {\n",
    "    \"70s\": rap_70_lyrics,\n",
    "    \"80s\": rap_80_lyrics,\n",
    "    \"90s\": rap_90_lyrics,\n",
    "    \"00s\": rap_00_lyrics,\n",
    "    \"10s\": rap_10_lyrics,\n",
    "    \"20s\": rap_20_lyrics\n",
    "}\n",
    "\n",
    "import csv\n",
    "\n",
    "data_for_csv = []\n",
    "\n",
    "for decade_name, decade in list_of_lyrics.items():\n",
    "    i = 0\n",
    "    for topics in decade:\n",
    "        i = i + 1\n",
    "        data_for_csv.append({\n",
    "            \"Method\": \"BiGrams\",\n",
    "            \"Genre\": \"Pop\",\n",
    "            \"Decade\": decade_name,\n",
    "            \"Topic\": str(decade[topics])\n",
    "        })\n",
    "        \n",
    "\n",
    "\n",
    "# Datei schreiben\n",
    "path = \"/home/maurice/Dokumente/3rd_Semester_LiU/TDDE16_Projekt/TDDE16_Song_Lyrics_Analysis/Analysis_Folder/LDA_BiGrams/\"\n",
    "\n",
    "output_file = path + \"topics_rap_LDA_1000_11_1.csv\"\n",
    "with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\"Method\", \"Genre\", \"Decade\", \"Topic\"])\n",
    "    writer.writeheader()  \n",
    "    writer.writerows(data_for_csv) \n",
    "\n",
    "print(f\"CSV-Datei '{output_file}' wurde erfolgreich erstellt!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d17fedf-8bf3-4661-b436-4ee4b3361548",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "type(rap_70_lyrics[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7599b719-6c7e-4a39-a10d-9afa5bf73fc6",
   "metadata": {},
   "source": [
    "# tipps von ChaGPT\n",
    "https://chatgpt.com/c/676f1ffc-b134-8012-9f03-6bd466bda148\n",
    "\n",
    "- Herausfinden, wieviele Topics am geeingesten sind\n",
    "- warum sind weiterhin so viele Stop words enthalten\n",
    "- wieviele Lieder nehm ich überhaupt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe2b480-22d5-4cf4-80e5-a80de53db53d",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "print(len(subdataset_80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082c6e5b-0527-44eb-9dc8-8f963f035875",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "print(len(subdataset_90))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac41e834-cb3b-42fc-8321-42e4b9050621",
   "metadata": {},
   "source": [
    "<h1> BERT TOPIC MODELLING </h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94fb4e4f-0e6c-4320-8c6a-6ed5dba60686",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/maurice/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/maurice/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import re\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import kagglehub\n",
    "import os\n",
    "\n",
    "\n",
    "# Sicherstellen, dass NLTK-Daten heruntergeladen sind\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# clean the text\n",
    "import re\n",
    "\n",
    "def clean_text(document_list):\n",
    "    \n",
    "    cleaned_document_list = []\n",
    "    \n",
    "    for single_doc in document_list:\n",
    "        # Delete HTML-Tags\n",
    "        text = re.sub(r\"<.*?>\", \"\", single_doc)\n",
    "        # Delete Sonderzeichen und Zahlen\n",
    "        text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "        # Delete uncessary whitespaces\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "        \n",
    "        # lower case the text\n",
    "        text = text.lower()\n",
    "        \n",
    "        cleaned_document_list.append(text)\n",
    "    \n",
    "    return cleaned_document_list\n",
    "\n",
    "def get_swear_words():\n",
    "\n",
    "    path = kagglehub.dataset_download(\"tushifire/ldnoobw\")\n",
    "    path = kagglehub.dataset_download(\"sahib12/badwords\")\n",
    "\n",
    "    #file = kagglehub.load_dataset(\"tushifire/ldnoobw\",path)\n",
    "\n",
    "    print(\"Path to dataset files:\", path)\n",
    "\n",
    "    swear_words_file = os.path.join(path, 'Terms-to-Block.csv')  # Replace 'en.txt' with the correct filename\n",
    "\n",
    "    with open(swear_words_file, 'r') as f:\n",
    "        swear_words = set(f.read().splitlines())\n",
    "\n",
    "    print(f\"!!!!Loaded {len(swear_words)} swear words.\")\n",
    "\n",
    "    return swear_words\n",
    "\n",
    "# Preprocessing: Tokenisieren, Stopwords entfernen, Kleinbuchstaben\n",
    "def preprocess_stop_words(document_list):\n",
    "    # Stopwords definieren\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))  # Standard-Stopwords\n",
    "    additional_words = {\"like\", \"oh\", \"na\", \"la\", \"yo\", \"you\", \"get\", \n",
    "                            \"the\", \"and\", \"of\", \"on\", \"[Intro]\", \"[Storyteller]\"}\n",
    "    \n",
    "    swear_words = get_swear_words()\n",
    "      \n",
    "    word_set = stop_words.union(additional_words)\n",
    "    word_set = word_set.union(swear_words)\n",
    "        \n",
    "    replacement = \"\"\n",
    "    list_of_tokens = []\n",
    "    for single_doc in document_list:\n",
    "        # Kombiniere Standard-Stopwords mit zusätzlichen Wörtern\n",
    "\n",
    "        # Splitte den Text in Wörter\n",
    "        words = single_doc.split()\n",
    "        \n",
    "        # Ersetze jedes Wort, falls es in der Liste vorkommt\n",
    "        replaced_words = []\n",
    "        for word in words:\n",
    "            if word.lower() in word_set:\n",
    "                replaced_words.append(replacement)  # Ersetze das Wort\n",
    "            else:\n",
    "                replaced_words.append(word)  # Behalte das Wort\n",
    "        \n",
    "        # Füge den Text wieder zusammen\n",
    "        list_of_tokens.append(str(\" \".join(replaced_words)))\n",
    "              \n",
    "    \n",
    "    return list_of_tokens\n",
    "\n",
    "def remove_repeated_lines(doc):\n",
    "    # doc ist eine Liste von Strings, z.B. [\"Zeile1\", \"Zeile2\", ...]\n",
    "    # Wiederholte Zeilen entfernen und Reihenfolge beibehalten\n",
    "    unique_lines = []\n",
    "    seen = set()\n",
    "    for line in doc:\n",
    "        if line not in seen:\n",
    "            unique_lines.append(line)\n",
    "            seen.add(line)\n",
    "    # Am Ende kannst du wieder eine Liste zurückgeben:\n",
    "    return unique_lines\n",
    "\n",
    "def tokenize_all_docs(document_list):\n",
    "    list_of_tokens = []\n",
    "    \n",
    "    for single_doc in document_list:\n",
    "        \n",
    "        if type(single_doc) == str:\n",
    "            tokens = word_tokenize(single_doc)\n",
    "            list_of_tokens.append(tokens)\n",
    "        else:\n",
    "            print(\"Type von Document\", type(single_doc))\n",
    "            continue\n",
    "    \n",
    "    return list_of_tokens\n",
    "\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6970a026-c9cf-4da3-bc7d-8368dfec0fe1",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import csv\n",
    "\n",
    "\n",
    "def processBertTopics(dataset,sentenceTransformer):\n",
    "    i = 0\n",
    "    # Initialize your topic model\n",
    "    #embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    \n",
    "    embedding_model = SentenceTransformer(sentenceTransformer)\n",
    "    topic_model = BERTopic(embedding_model=embedding_model)\n",
    "    # Fit and transform the text data\n",
    "    #without_repatition = remove_repeated_lines(dataset)\n",
    "    \n",
    "    # clean dataset from HTML-Tags, special characters and numbers\n",
    "    cleaned_song_lyrics = clean_text(dataset)\n",
    "    \n",
    "    # Get rid of the bad words and replace it with NOTHING\n",
    "    swear_words = set(stopwords.words('english'))\n",
    "    #without_swear = [sort_out_bad_words(doc) for doc in cleaned_song_lyrics]\n",
    "    \n",
    "    #Deleting the stopwords\n",
    "    without_stop_words = preprocess_stop_words(cleaned_song_lyrics)\n",
    "    \n",
    "    tokenized_docs = tokenize_all_docs(without_stop_words)\n",
    "    \n",
    "    # Convert 2 strings\n",
    "    tokenized_docs = [\" \".join(tokens) for tokens in tokenized_docs]\n",
    "\n",
    "    # Remove empty strings\n",
    "    tokenized_docs = [doc for doc in tokenized_docs if doc]\n",
    "    \n",
    "    # Remove documents with less than 3 words    \n",
    "    #tokenized_docs = [doc for doc in tokenized_docs if len(doc.split()) > 3]\n",
    "    #print(\"tokenized_docs\", tokenized_docs)\n",
    "\n",
    "    if tokenized_docs == [] or len(tokenized_docs) == 0:\n",
    "        i = i+1\n",
    "    else: \n",
    "        topics, probs = topic_model.fit_transform(tokenized_docs)\n",
    "\n",
    "    # Ergebnisse\n",
    "    #print(\"Themenzuweisung:\", topics)\n",
    "    #print(\"Wahrscheinlichkeiten:\", probs)\n",
    "\n",
    "    str_topic_with_probs = []\n",
    "    #for t, p in zip(topics, probs):\n",
    "    #    str_topic_with_probs.append(str(t) + \": \" + str(p))\n",
    "\n",
    "    print(i, \"documents were empty!\")   \n",
    "    return topic_model.get_topics()\n",
    "\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4dd6ae7c-546b-4eb3-8f54-d43536abfbad",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Verarbeitung der Eingabe durch das Modell\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 18\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mbertweet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Models outputs sind jetzt Tupel\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Ausgabe der Features (optional: erster Layer oder hidden states)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(features\u001b[38;5;241m.\u001b[39mlast_hidden_state)  \u001b[38;5;66;03m# Beispielausgabe\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:912\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    910\u001b[0m         token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m--> 912\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    914\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    921\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones((batch_size, seq_length \u001b[38;5;241m+\u001b[39m past_key_values_length), device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:127\u001b[0m, in \u001b[0;36mRobertaEmbeddings.forward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    125\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m token_type_embeddings\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabsolute\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 127\u001b[0m     position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     embeddings \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m position_embeddings\n\u001b[1;32m    129\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(embeddings)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Modell und Tokenizer laden\n",
    "bertweet = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False)\n",
    "\n",
    "# Anzahl der Songtexte, die verarbeitet werden sollen\n",
    "amount = 10  # Beispiel: Anzahl der Texte, die verarbeitet werden sollen\n",
    "\n",
    "# INPUT TWEET IST BEREITS NORMALISIERT!\n",
    "# Tokenisierung und Padding/Truncation\n",
    "lyrics = subdataset_90['lyrics'][:amount]\n",
    "inputs = tokenizer(lyrics, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Verarbeitung der Eingabe durch das Modell\n",
    "with torch.no_grad():\n",
    "    features = bertweet(**inputs)  # Models outputs sind jetzt Tupel\n",
    "\n",
    "# Ausgabe der Features (optional: erster Layer oder hidden states)\n",
    "print(features.last_hidden_state)  # Beispielausgabe\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad529d32-bc48-4ec2-b26f-7cb3e1c0ff3d",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import datetime\n",
    "from datasets import load_from_disk\n",
    "import os\n",
    "def run_analytics_bert_different_Sentence_Transformers(amount_per_iteration,name_of_run, sentence_transformer):\n",
    "\n",
    "    start_time = time.time()\n",
    "    amount = amount_per_iteration\n",
    "\n",
    "    path = \"/home/maurice/Dokumente/3rd_Semester_LiU/TDDE16_Projekt/TDDE16_Song_Lyrics_Analysis/Subdatasets/rap/\"\n",
    "\n",
    "    \n",
    "    rap_70_lyrics_bert = processBertTopics(load_from_disk(path + 'subdataset_70_rap')['lyrics'][:amount],sentence_transformer)\n",
    "    rap_80_lyrics_bert = processBertTopics(load_from_disk(path + \"subdataset_80_rap\")['lyrics'][:amount],sentence_transformer)\n",
    "    rap_90_lyrics_bert = processBertTopics(load_from_disk(path + \"subdataset_90_rap\")['lyrics'][:amount],sentence_transformer)\n",
    "    rap_00_lyrics_bert = processBertTopics(load_from_disk(path + \"subdataset_00_rap\")['lyrics'][:amount],sentence_transformer)\n",
    "    rap_10_lyrics_bert = processBertTopics(load_from_disk(path + \"subdataset_10_rap\")['lyrics'][:amount],sentence_transformer)\n",
    "    rap_20_lyrics_bert = processBertTopics(load_from_disk(path + \"subdataset_20_rap\")['lyrics'][:amount],sentence_transformer)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    list_of_lyrics_bert = {\n",
    "        \"70s\": rap_70_lyrics_bert,\n",
    "        \"80s\": rap_80_lyrics_bert,\n",
    "        \"90s\": rap_90_lyrics_bert,\n",
    "        \"00s\": rap_00_lyrics_bert,\n",
    "        \"10s\": rap_10_lyrics_bert,\n",
    "        \"20s\": rap_20_lyrics_bert,\n",
    "    }\n",
    "    end_time = time.time()\n",
    "\n",
    "    amount_of_time = end_time - start_time\n",
    "    \n",
    "    data_for_csv_bert = []\n",
    "    \n",
    "    for decade_name, decade in list_of_lyrics_bert.items():\n",
    "        i = 0\n",
    "        \n",
    "        for topics in decade:\n",
    "            i = i + 1\n",
    "            data_for_csv_bert.append({\n",
    "                \"Method\": \"BERT\",\n",
    "                \"Genre\": \"Rap\",\n",
    "                \"Decade\": decade_name,\n",
    "                \"Topic\": str(decade[topics])\n",
    "            })\n",
    "    data_for_csv_bert.append({\n",
    "                \"Method\": \"TIME\",\n",
    "                \"Genre\": \"TIME\",\n",
    "                \"Decade\": \"TIME\",\n",
    "                \"Topic\": str(amount_of_time)\n",
    "            })\n",
    "    \n",
    "    # Write to csv\n",
    "    now = datetime.datetime.now()\n",
    "    path = \"/home/maurice/Dokumente/3rd_Semester_LiU/TDDE16_Projekt/TDDE16_Song_Lyrics_Analysis/Analysis_Folder/BERT/Rap/\"\n",
    "    \n",
    "    # Datei schreiben\n",
    "    name_of_file = \"topics_RAP_BERT_\" + \"6\"+\":\"+ \"22\"+ \":07\" + \"_MiniLM.csv\"\n",
    "    output_file = os.path.join(path, name_of_file)\n",
    "    \n",
    "\n",
    "    with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\"Method\", \"Genre\", \"Decade\", \"Topic\"])\n",
    "        writer.writeheader()  \n",
    "        writer.writerows(data_for_csv_bert) \n",
    "   \n",
    "    print(f\"CSV-Datei '{output_file}' wurde erfolgreich erstellt!\")\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c70ccbf7-d026-4549-a865-36a22b423889",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/maurice/.cache/kagglehub/datasets/sahib12/badwords/versions/1\n",
      "!!!!Loaded 2130 swear words.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/numba/np/ufunc/parallel.py:371: NumbaWarning: \u001b[1mThe TBB threading layer requires TBB version 2021 update 6 or later i.e., TBB_INTERFACE_VERSION >= 12060. Found TBB_INTERFACE_VERSION = 12050. The TBB threading layer is disabled.\u001b[0m\n",
      "  warnings.warn(problem)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 documents were empty!\n",
      "Path to dataset files: /home/maurice/.cache/kagglehub/datasets/sahib12/badwords/versions/1\n",
      "!!!!Loaded 2130 swear words.\n",
      "0 documents were empty!\n",
      "Path to dataset files: /home/maurice/.cache/kagglehub/datasets/sahib12/badwords/versions/1\n",
      "!!!!Loaded 2130 swear words.\n",
      "0 documents were empty!\n",
      "Path to dataset files: /home/maurice/.cache/kagglehub/datasets/sahib12/badwords/versions/1\n",
      "!!!!Loaded 2130 swear words.\n",
      "0 documents were empty!\n",
      "Path to dataset files: /home/maurice/.cache/kagglehub/datasets/sahib12/badwords/versions/1\n",
      "!!!!Loaded 2130 swear words.\n",
      "0 documents were empty!\n",
      "Path to dataset files: /home/maurice/.cache/kagglehub/datasets/sahib12/badwords/versions/1\n",
      "!!!!Loaded 2130 swear words.\n",
      "0 documents were empty!\n",
      "CSV-Datei '/home/maurice/Dokumente/3rd_Semester_LiU/TDDE16_Projekt/TDDE16_Song_Lyrics_Analysis/Analysis_Folder/BERT/Rap/topics_RAP_BERT_6:22:07_MiniLM.csv' wurde erfolgreich erstellt!\n"
     ]
    }
   ],
   "source": [
    "run_analytics_bert_different_Sentence_Transformers(1000,\"BERT_mit_stop_words\",\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5c2ee952-5a1e-48d5-bc2d-fedd50cd225d",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'topic_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtopic_model\u001b[49m\u001b[38;5;241m.\u001b[39mget_topics(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'topic_model' is not defined"
     ]
    }
   ],
   "source": [
    "topic_model.get_topics(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d608b690",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
