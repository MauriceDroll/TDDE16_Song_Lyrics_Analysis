{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c664878-4aea-49c8-bdcd-0c6a10e83c42",
   "metadata": {},
   "source": [
    "<h1> GENRE: RAP </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb58b5aa-e4aa-47ea-8de6-39736199bc11",
   "metadata": {},
   "source": [
    "<h3>(1) Access the data from the dataset </h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c36f509d-632c-432c-853f-e7dfee496973",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install datasets\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"sebastiandizon/genius-song-lyrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a84645ae-05c1-47c2-af63-c43cdf167dca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "986b9115aded473bbc15df3ad4765cba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/5134856 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f332c4f5da77487e93ca25d53c7f1fef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/5134856 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b06c43a8ac334ad4b07a336af9d0160d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/5134856 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "360c6c7abb0c4426a61c993d23b20c4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/5134856 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05042eacecca4306a69de658fa008a28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/5134856 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ccdae6a541b4931a35ad74573d2cba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/5134856 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e26cd466e8424ca9841302c99289ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/5134856 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assuming the dataset has a 'category' column\n",
    "unique_categories = ds['train'].unique('tag')  # Get all unique categories\n",
    "\n",
    "# Group into subdatasets\n",
    "subdatasets = {category: ds['train'].filter(lambda x: x['tag'] == category and x['language'] == 'en') for category in unique_categories}\n",
    "\n",
    "type_of_music = 'rap'\n",
    "# Group into music decades\n",
    "subdataset_70 = ds['train'].filter(lambda song: 1960 <= song[\"year\"] <= 1970 and song['tag'] == type_of_music)\n",
    "subdataset_80 = ds['train'].filter(lambda song: 1970 <= song[\"year\"] <= 1980 and song['tag'] == type_of_music)\n",
    "subdataset_90 = ds['train'].filter(lambda song: 1980 <= song[\"year\"] <= 1990 and song['tag'] == type_of_music)\n",
    "subdataset_00 = ds['train'].filter(lambda song: 1990 <= song[\"year\"] <= 2000 and song['tag'] == type_of_music)\n",
    "subdataset_10 = ds['train'].filter(lambda song: 2000 <= song[\"year\"] <= 2010 and song['tag'] == type_of_music)\n",
    "subdataset_20 = ds['train'].filter(lambda song: 2010 <= song[\"year\"] <= 2020 and song['tag'] == type_of_music)\n",
    "subdataset_30 = ds['train'].filter(lambda song: 2020 <= song[\"year\"] <= 2030 and song['tag'] == type_of_music)\n",
    "\n",
    "# Create subdataset\n",
    "#eng_rap = subdatasets['rap']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eca33b8-ff6c-4ab3-aae9-8de32db0f00e",
   "metadata": {},
   "source": [
    "<h2> LDA </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6c1930-379d-4c13-8378-0c7e31382765",
   "metadata": {},
   "source": [
    "<h2> Remove the bad words</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc19c9ee-4499-49cb-ba77-32ff6fa3fb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/maurice/.cache/kagglehub/datasets/tushifire/ldnoobw/versions/2\n",
      "!!!!Loaded 403 swear words.\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import os\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"tushifire/ldnoobw\")\n",
    "#file = kagglehub.load_dataset(\"tushifire/ldnoobw\",path)\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "swear_words_file = os.path.join(path, 'en')  # Replace 'en.txt' with the correct filename\n",
    "\n",
    "with open(swear_words_file, 'r') as f:\n",
    "    swear_words = set(f.read().splitlines())\n",
    "\n",
    "print(f\"!!!!Loaded {len(swear_words)} swear words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cecd6a07-2735-4e22-8936-6201e2f4454e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/maurice/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/maurice/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import re\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "# Sicherstellen, dass NLTK-Daten heruntergeladen sind\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Beispiel-Liste von Strings\n",
    "\n",
    "\n",
    "# Stopwords definieren\n",
    "stop_words = set(stopwords.words('english')+[\"like\", \"oh\", \"na\", \"la\", \"yo\", \"get\"])\n",
    "\n",
    "# Preprocessing: Tokenisieren, Stopwords entfernen, Kleinbuchstaben\n",
    "def preprocess(doc):\n",
    "    doc = re.sub(r\"[^a-zA-Z0-9\\s]\", \" \", doc) # delete unncessary things\n",
    "    #doc = remove_stopwords(doc.lower())\n",
    "    doc = remove_repeated_lines(doc)\n",
    "    \n",
    "    tokens = word_tokenize(doc.lower())  # Kleinbuchstaben und Tokenisierung\n",
    "    \n",
    "    tokens = [word for word in tokens if word not in stop_words]  # Stopwords entfernen\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def remove_repeated_lines(doc):\n",
    "    lines = doc.split(\"\\n\")\n",
    "    return \"\\n\".join(sorted(set(lines), key=lines.index))\n",
    "\n",
    "def sort_out_bad_words(doc,word_list, replacement=\"swear\"):\n",
    "    \"\"\"\n",
    "    Ersetzt Wörter in einem Text, wenn sie in einer Liste vorkommen.\n",
    "    \n",
    "    :param text: Der Eingabetext (String).\n",
    "    :param word_list: Liste der Wörter, die ersetzt werden sollen.\n",
    "    :param replacement: Der Text, durch den die Wörter ersetzt werden sollen.\n",
    "    :return: Der bearbeitete Text.\n",
    "    \"\"\"\n",
    "    # Erstelle ein Set für einen schnelleren Lookup\n",
    "    word_set = set(word_list)\n",
    "    \n",
    "    # Splitte den Text in Wörter\n",
    "    words = doc.split()\n",
    "    \n",
    "    # Ersetze jedes Wort, falls es in der Liste vorkommt\n",
    "    replaced_words = [replacement if word.lower() in word_set else word for word in words]\n",
    "    \n",
    "    # Füge den Text wieder zusammen\n",
    "    return \" \".join(replaced_words)\n",
    " \n",
    "\n",
    "def process_subdatasets(subdataset):\n",
    "    # Get rid of the bad words and replace it with \"swear word\"\n",
    "    without_swear = [sort_out_bad_words(doc, swear_words ) for doc in subdataset]\n",
    "\n",
    "    # Preprocessing für alle Dokumente anwenden\n",
    "    processed_docs = [preprocess(doc) for doc in without_swear]\n",
    "\n",
    "    # Erstellen eines Wörterbuchs\n",
    "    dictionary = Dictionary(processed_docs)\n",
    "\n",
    "    # Erstellen eines Bag-of-Words-Korpus\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "\n",
    "    # LDA-Modell trainieren\n",
    "    lda_model = gensim.models.LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=dictionary,\n",
    "        num_topics=5,  # Anzahl der Themen\n",
    "        random_state=42,\n",
    "        passes=40,  # Anzahl der Durchgänge\n",
    "        alpha='symmetric',\n",
    "        eta='auto'\n",
    "        #alpha='auto'  # Dirichlet-Hyperparameter Entscheidung zwischen Auto und symetric\n",
    "    )\n",
    "    \n",
    "    id2word = dictionary\n",
    "    # Themen anzeigen\n",
    "    print(\"Themen und ihre Schlüsselwörter:\")\n",
    "    for idx, topic in lda_model.print_topics(num_words=5):\n",
    "        print(f\"Topic {idx}: {topic}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "750121e6-67ce-4d46-bfb2-08910881b4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Themen und ihre Schlüsselwörter:\n",
      "Topic 0: 0.040*\"yuh\" + 0.038*\"pib\" + 0.038*\"flosisisnsn\" + 0.038*\"bsew\" + 0.014*\"got\"\n",
      "Topic 1: 0.019*\"hey\" + 0.014*\"got\" + 0.011*\"yeah\" + 0.009*\"fortnite\" + 0.008*\"real\"\n",
      "Topic 2: 0.027*\"niggers\" + 0.014*\"swear\" + 0.012*\"revolution\" + 0.008*\"comes\" + 0.008*\"change\"\n",
      "Topic 3: 0.054*\"feat\" + 0.012*\"swear\" + 0.011*\"lil\" + 0.005*\"denzel\" + 0.005*\"curry\"\n",
      "Topic 4: 0.021*\"swear\" + 0.015*\"kevin\" + 0.008*\"running\" + 0.008*\"bitch\" + 0.007*\"time\"\n",
      "Themen und ihre Schlüsselwörter:\n",
      "Topic 0: 0.009*\"hey\" + 0.008*\"yeah\" + 0.008*\"know\" + 0.008*\"verse\" + 0.007*\"scott\"\n",
      "Topic 1: 0.027*\"jam\" + 0.012*\"j\" + 0.009*\"jibbit\" + 0.008*\"le\" + 0.007*\"que\"\n",
      "Topic 2: 0.037*\"mi\" + 0.024*\"say\" + 0.023*\"di\" + 0.012*\"fi\" + 0.011*\"cah\"\n",
      "Topic 3: 0.027*\"rock\" + 0.014*\"got\" + 0.012*\"beat\" + 0.011*\"said\" + 0.011*\"say\"\n",
      "Topic 4: 0.020*\"swear\" + 0.009*\"time\" + 0.008*\"love\" + 0.008*\"black\" + 0.008*\"man\"\n",
      "Themen und ihre Schlüsselwörter:\n",
      "Topic 0: 0.013*\"got\" + 0.012*\"swear\" + 0.011*\"know\" + 0.011*\"love\" + 0.011*\"said\"\n",
      "Topic 1: 0.021*\"rock\" + 0.014*\"got\" + 0.012*\"let\" + 0.010*\"say\" + 0.010*\"yeah\"\n",
      "Topic 2: 0.057*\"mi\" + 0.031*\"di\" + 0.028*\"say\" + 0.017*\"dem\" + 0.015*\"fi\"\n",
      "Topic 3: 0.013*\"cause\" + 0.007*\"verse\" + 0.006*\"know\" + 0.006*\"got\" + 0.006*\"one\"\n",
      "Topic 4: 0.058*\"gang\" + 0.017*\"er\" + 0.016*\"r\" + 0.014*\"en\" + 0.014*\"de\"\n",
      "Themen und ihre Schlüsselwörter:\n",
      "Topic 0: 0.017*\"got\" + 0.014*\"know\" + 0.014*\"yeah\" + 0.010*\"baby\" + 0.010*\"let\"\n",
      "Topic 1: 0.026*\"de\" + 0.017*\"l\" + 0.016*\"le\" + 0.014*\"les\" + 0.010*\"est\"\n",
      "Topic 2: 0.009*\"swear\" + 0.006*\"verse\" + 0.005*\"one\" + 0.005*\"rock\" + 0.005*\"mc\"\n",
      "Topic 3: 0.067*\"swear\" + 0.020*\"niggas\" + 0.016*\"got\" + 0.009*\"ya\" + 0.008*\"know\"\n",
      "Topic 4: 0.013*\"got\" + 0.009*\"life\" + 0.009*\"know\" + 0.008*\"cause\" + 0.008*\"one\"\n",
      "Themen und ihre Schlüsselwörter:\n",
      "Topic 0: 0.058*\"swear\" + 0.019*\"niggas\" + 0.014*\"got\" + 0.011*\"ya\" + 0.008*\"know\"\n",
      "Topic 1: 0.018*\"got\" + 0.013*\"money\" + 0.009*\"yeah\" + 0.007*\"big\" + 0.006*\"know\"\n",
      "Topic 2: 0.006*\"verse\" + 0.006*\"life\" + 0.006*\"one\" + 0.006*\"got\" + 0.005*\"never\"\n",
      "Topic 3: 0.019*\"know\" + 0.018*\"got\" + 0.013*\"go\" + 0.013*\"yeah\" + 0.013*\"swear\"\n",
      "Topic 4: 0.025*\"de\" + 0.017*\"les\" + 0.016*\"l\" + 0.016*\"j\" + 0.016*\"le\"\n",
      "Themen und ihre Schlüsselwörter:\n",
      "Topic 0: 0.049*\"swear\" + 0.016*\"got\" + 0.012*\"niggas\" + 0.008*\"ya\" + 0.008*\"know\"\n",
      "Topic 1: 0.017*\"que\" + 0.016*\"da\" + 0.013*\"mi\" + 0.010*\"di\" + 0.009*\"dem\"\n",
      "Topic 2: 0.015*\"ich\" + 0.015*\"j\" + 0.014*\"de\" + 0.011*\"l\" + 0.011*\"du\"\n",
      "Topic 3: 0.004*\"rap\" + 0.004*\"black\" + 0.004*\"one\" + 0.003*\"man\" + 0.003*\"us\"\n",
      "Topic 4: 0.014*\"got\" + 0.013*\"know\" + 0.008*\"cause\" + 0.008*\"verse\" + 0.008*\"love\"\n",
      "Themen und ihre Schlüsselwörter:\n",
      "Topic 0: 0.027*\"eu\" + 0.024*\"que\" + 0.021*\"n\" + 0.019*\"e\" + 0.017*\"de\"\n",
      "Topic 1: 0.027*\"w\" + 0.024*\"nie\" + 0.013*\"si\" + 0.012*\"jak\" + 0.010*\"e\"\n",
      "Topic 2: 0.030*\"j\" + 0.021*\"de\" + 0.020*\"que\" + 0.014*\"est\" + 0.014*\"en\"\n",
      "Topic 3: 0.014*\"n\" + 0.013*\"ich\" + 0.011*\"r\" + 0.008*\"du\" + 0.008*\"ja\"\n",
      "Topic 4: 0.033*\"swear\" + 0.018*\"yeah\" + 0.016*\"got\" + 0.011*\"know\" + 0.007*\"verse\"\n",
      "CSV-Datei 'topics_rap_LDA.csv' wurde erfolgreich erstellt!\n"
     ]
    }
   ],
   "source": [
    "rap_70_lyrics = process_subdatasets(subdataset_70['lyrics'][:10000])\n",
    "rap_80_lyrics = process_subdatasets(subdataset_80['lyrics'][:10000])\n",
    "rap_90_lyrics = process_subdatasets(subdataset_90['lyrics'][:10000])\n",
    "rap_00_lyrics = process_subdatasets(subdataset_00['lyrics'][:10000])\n",
    "rap_10_lyrics = process_subdatasets(subdataset_10['lyrics'][:10000])\n",
    "rap_20_lyrics = process_subdatasets(subdataset_20['lyrics'][:10000])\n",
    "rap_30_lyrics = process_subdatasets(subdataset_30['lyrics'][:10000])\n",
    "\n",
    "import csv\n",
    "\n",
    "# Daten für die CSV\n",
    "data = [\n",
    "    {\"Method\": \"LDA\", \"Genre\": \"Rap\", \"Decade\": \"1970s\",\"Topic\": rap_70_lyrics},\n",
    "    {\"Method\": \"LDA\", \"Genre\": \"Rap\", \"Decade\": \"1980s\",\"Topic\": rap_80_lyrics},\n",
    "    {\"Method\": \"LDA\", \"Genre\": \"Rap\", \"Decade\": \"1990s\",\"Topic\": rap_90_lyrics},\n",
    "    {\"Method\": \"LDA\", \"Genre\": \"Rap\", \"Decade\": \"2000s\",\"Topic\": rap_00_lyrics},\n",
    "    {\"Method\": \"LDA\", \"Genre\": \"Rap\", \"Decade\": \"2010s\",\"Topic\": rap_10_lyrics},\n",
    "    {\"Method\": \"LDA\", \"Genre\": \"Rap\", \"Decade\": \"2020s\",\"Topic\": rap_20_lyrics},\n",
    "    {\"Method\": \"LDA\", \"Genre\": \"Rap\", \"Decade\": \"2030s\",\"Topic\": rap_30_lyrics},\n",
    "]\n",
    "\n",
    "# Datei schreiben\n",
    "output_file = \"topics_rap_LDA.csv\"\n",
    "with open(output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\"Method\", \"Genre\", \"Decade\", \"Topic\"])\n",
    "    writer.writeheader()  # Schreibe die Kopfzeile\n",
    "    writer.writerows(data)  # Schreibe die Daten\n",
    "\n",
    "print(f\"CSV-Datei '{output_file}' wurde erfolgreich erstellt!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7599b719-6c7e-4a39-a10d-9afa5bf73fc6",
   "metadata": {},
   "source": [
    "# tipps von ChaGPT\n",
    "https://chatgpt.com/c/676f1ffc-b134-8012-9f03-6bd466bda148\n",
    "\n",
    "- Herausfinden, wieviele Topics am geeingesten sind\n",
    "- warum sind weiterhin so viele Stop words enthalten\n",
    "- wieviele Lieder nehm ich überhaupt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0fe2b480-22d5-4cf4-80e5-a80de53db53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "440\n"
     ]
    }
   ],
   "source": [
    "print(len(subdataset_80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "082c6e5b-0527-44eb-9dc8-8f963f035875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4505\n"
     ]
    }
   ],
   "source": [
    "print(len(subdataset_90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c89478d-f119-430e-9ea5-337c8d7d4334",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
